\documentclass{assignment}
\UsingEnglish
\ProjectInfos*{Intro to Communication System}{EE140}{Fall, 2020}{Assignment 8}{Due time : 10:15, Nov 27, 2020 (Friday)}{陈稼霖}{45875852}
\begin{document}
\begin{prob}[2.11]
    Proof of the Kraft inequality for uniquely decodable codes.
    \begin{itemize}
        \item[(a)] Assume a uniquely decodable code has lengths $l_1,\cdots l_M$. In order to show that $\sum_j2^{-l_j}\leq 1$, demonstrate the following identity for each integer $n\geq 1$:
        \[
            \left[\sum_{j=1}^M2^{-l_j}\right]^n=\sum_{j_1=1}^M\sum_{j_2=1}^M\cdots\sum_{j_n=1}^M2^{-(l_{j_1}+l_{j_2}+\cdots l_{j_n})}.
        \]
        \item[(b)] Show that there is one term on the right for each concatenation of $n$ codewords (i.e. for the encoding of one $n$-tuple $\bm{x}^n$) where $l_{j_1}+l_{j_2}+\cdots+l_{j_n}$ is the aggregate length of that concatenation.
        \item[(c)] Let $A_i$ be the number of concatenations which have overall length $i$ and show that
        \[
            \left[\sum_{j=1}^M2^{-l_j}\right]^n=\sum_{i=1}^{nl_{max}}A_i2^{-i}.
        \]
        \item[(d)] Using the unique decodablility, upperbound each $A_i$ and show that
        \[
            \left[\sum_{j=1}^M2^{-l_j}\right]^n\leq nl_{\max}.
        \]
        \item[(e)] By taking the $n$th root and letting $n\rightarrow\infty$, demonstrate the Kraft inequality.
    \end{itemize}
\end{prob}
\begin{pf}
    \begin{itemize}
        \item[(a)] 
        \begin{align}
            \left[\sum_{j=1}M2^{-l_j}\right]^n=\left[\sum_{j_1=1}^M2^{-l_{j_1}}\right]\left[\sum_{j_2=1}^M2^{-l_{j_2}}\right]\cdots\left[\sum_{j_n=1}^M2^{-l_{j_n}}\right]=\sum_{j_1=1}^M\sum_{j_2=1}^M\cdots\sum_{j_n=1}^M2^{-(l_{j_1}+l_{j_2}+\cdots+l_{j_n})},\quad\forall n\geq 1.
        \end{align}
        \item[(b)] For each concatenation of $n$ codewords $\bm{x}^n$, the length of the $k$th codeword is $l_{j_k}$ and there is $n$ codewords in total ($1\leq k\leq n$), so the aggregate length of the concatenation is $l_{j_1}+l_{j_2}+\cdots+l_{j_n}$.
        \item[(c)] Using the conclusion we obtained in (b), we have
        \begin{align}
            \left[\sum_{j=1}^M2^{-l_j}\right]^n=\sum_{\bm{x}^n}e^{-(\text{length of }\bm{x}^n)}=\sum_{i=1}^{nl_{\max}}A_i2^{-i}.
        \end{align}
        \item[(d)] Because of unique decodablility (i.e. each concatenation should be different),
        \begin{align}
            A_i\leq 2^i,\quad\forall i.
        \end{align}
        Thus,
        \begin{align}
            \label{P-1-d}
            \left[\sum_{j=1}^M2^{-l_j}\right]^n=\sum_{i=1}^{nl_{\max}}A_i2^{-i}\leq\sum_{i=1}^{nl_{\max}}2^i2^{-i}=\sum_{i=1}^{nl_{\max}}1=nl_{\max}.
        \end{align}
        \item[(e)] Taking the $n$th root of equation \eqref{P-1-d}, we have
        \begin{align}
            \sum_{j=1}^M2^{-l_j}\leq[nl_{\max}]^{1/n}.
        \end{align}
        Letting $n\rightarrow\infty$, we have
        \begin{align}
            \sum_{j=1}^M2^{-l_j}\leq\lim_{n\rightarrow\infty}[nl_{\max}]^{1/n}=1,
        \end{align}
        which is the Kraft inequality.
    \end{itemize}
\end{pf}

\begin{prob}[2.12]
    A source with an alphabet size of $M=\abs{\mathcal{X}}=4$ has a symbol probabilities $\{1/3,1/3,2/9,1/9\}$.
    \begin{itemize}
        \item[(a)] Use the Huffman algorithm to find an optimal prefix-free code for this source.
        \item[(b)] Use the Huffman algorithm to find another optimal prefix-free code with a different set of lengths.
        \item[(c)] Find another prefix-free code that is optimal but cannot result from using the Huffman algorithm.
    \end{itemize}
\end{prob}
\begin{sol}
    \begin{itemize}
        Suppose the four symbols corresponding to the probabilities $\{1/3,1/3,2/9,1/9\}$ are a,b,c,d.
        \item[(a)] An optimal prefix-free code for this source is shown in figure \ref{A-8-P-2-a}.
        \item[(b)] Another optimal prefix-free code for this source is shown in figure \ref{A-8-P-2-b}.
        \begin{figure}[h]
            \centering
            \subfigure[An optimal prefix-free code derived from the Huffman algorithm.]{
                \includegraphics[width=.45\columnwidth]{A-8-P-2-a.pdf}
                \label{A-8-P-2-a}
            }
            \subfigure[Another optimal prefix-free code derived from the Huffman algorithm.]{
                \includegraphics[width=.45\columnwidth]{A-8-P-2-b.pdf}
                \label{A-8-P-2-b}
            }
            \caption{Two optimal prefix-free code scheme.}
        \end{figure}
            \item[(c)] Another prefix-free code that is optimal but cannot result from using the Huffman algorithm is shown in table \ref{A-8-P-2-c}.
            \begin{table}[H]
                \centering
                \caption{Another prefix-free code that is optimal but cannot result from using the Huffman algorithm}
                \label{A-8-P-2-c}
                \begin{tabular}{|c|c|}
                \hline
                symbol & codeword \\ \hline
                a & 00 \\ \hline
                b & 11 \\ \hline
                c & 10 \\ \hline
                d & 01 \\ \hline
                \end{tabular}
            \end{table}
    \end{itemize}
\end{sol}

\begin{prob}[2.14]
    Consider a source with $M$ equiprobable symbols.
    \begin{itemize}
        \item[(a)] Let $k=\lceil\log M\rceil$. Show that, for a Huffman code, the only possible codeword lengths are $k$ and $k-1$.
        \item[(b)] As a function of $M$, find how many codewords have length $k=\lceil\log M\rceil$. What is the expected codeword length $\bar{L}$ in bits per source code?
        \item[(c)] Define $y=M/2^k$. Express $\bar{L}-\log M$ as a function of $y$. Find the maximum value of this function over $1/2<y\leq 1$. This illustrates that the entropy bound, $\bar{L}=H[X]+1$, is rather loose in this equiprobable case.
    \end{itemize}
\end{prob}
\begin{sol}
    \begin{itemize}
        \item[(a)] For a Huffman code, if $M$ is a power of $2$, say $M=2^k$, then the Huffman tree should be a full binary tree and the length is $k=\log_2M$ for all codewords. If $M$ is not a power of $2$, say $M=2^{k-1}+k_0$ where $0<k_0\leq 2^{k-1}$, the Huffman tree should be a complete but not full binary tree. In this case, some codewords are at the bottom layer of the Huffman tree whose lengths are all $k=\lceil\log_2M\rceil$, other codewords are at the bottom but one layer whose lengths are all $k-1=\lceil\log_2M\rceil-1$.
        \item[(b)] Suppose the number of codewords with length $k$ is $x$, then the number of codewords with length $k-1$ is $M-x$. The number of node at the bottom but one layer of the Huffman tree should be
        \begin{align}
            \frac{x}{2}+(M-x)=2^{k-1},
        \end{align}
        so the number of codeword with length $k$ is
        \begin{align}
            x=2M-2^k.
        \end{align}
        The expected code length per source code is
        \begin{align}
            \bar{L}=\frac{k(2M-2^k)+(k-1)(2^k-M)}{M}=k+1-\frac{2^k}{M}.
        \end{align}
        \item[(c)] Using $k=\log_2\frac{M}{y}$, we have
        \begin{align}
            \bar{L}-\log_2M=-\log_2y+1-\frac{1}{y}.
        \end{align}
        The derivative of the above function is
        \begin{align}
            \frac{\mathrm{d}}{\mathrm{d}y}=-\frac{1}{y\ln 2}+\frac{1}{y^2}\left\{\begin{array}{ll}
                >0,&\frac{1}{2}\leq y<\ln 2,\\
                <0,&\ln 2<y<1,
            \end{array}\right.
        \end{align}
        so the maximum value of the above function is
        \begin{align}
            \left[\bar{L}-\log_2M\right]_{\max}=\left[\bar{L}-\log_2M\right]_{y=\ln 2}=-\log_2(\ln 2)+1-\frac{1}{\ln 2}=0.086,
        \end{align}
        which means that
        \begin{align}
            \bar{L}\leq\log_2M+0.086\leq H(X)+0.086.
        \end{align}
        Therefore, the entropy bound $\bar{L}=H[X]+1$, is rather loose in this equiprobable case.
    \end{itemize}
\end{sol}

\begin{prob}[2.21]
    A discrete memoryless source emits iid random symbols $X_1,X_2,\cdots$ Each random symbol $X$ has the symbols $\{a,b,c\}$ with probabilities $\{0.5,0.4,0.1\}$, respectively.
    \begin{itemize}
        \item[(a)] Find the expected length $\bar{L}_{\min}$ of the best variable-length prefix-free code for $X$.
        \item[(b)] Find the expected length $\bar{L}_{\min,2}$, normalized to bits per symbol, of the best variable-length prefix-free code for $X^2$.
        \item[(c)] Is it true that for any DMS, $\bar{L}_{\min}\geq\bar{L}_{\min,2}$? Explain your answer.
    \end{itemize}
\end{prob}
\begin{sol}
    \begin{itemize}
        \item[(a)] The best variable prefix-free code for $X$ is shown in figure \ref{A-8-P-4-a}, whose expected length is
        \begin{align}
            \bar{L}_{\min}=0.5\times 1+0.4\times 2+0.1\times 2=1.5\quad.
        \end{align}
        \item[(b)] The best variable prefix-free code for $X^2$ is shown in figure \ref{A-8-P-4-b}, whose expected length normalized to bits per symbol is
        \begin{align}
            \notag\bar{L}_{\min,2}=&\frac{1}{2}(0.25\times 2+0.2\times 2+0.2\times 2+0.16\times 3+0.05\times 5+0.05\times 5+0.04\times 5+0.04\times 6+0.01\times 6)\\
            =&1.39\quad.
        \end{align}
        \item[(c)] It is true that for any DMS, $\bar{L}_{\min}\geq\bar{L}_{\min,2}$. One method for source coding of $X^2$ is to use the concatenation of two codewords of $X$ as the codeword of $X^2$, whose expected length per symbol equals $L_{\min}$. Because this method is not necessarily the best coding method, $L_{\min}$ can not be less than $\bar{L}_{\min,2}$, which is the minimal expected length per symbol of $X^2$.
        \begin{figure}[h]
            \centering
            \subfigure[The best variable-length prefix-free code for $X$]{
                \includegraphics[width=.45\columnwidth]{A-8-P-4-a.pdf}
                \label{A-8-P-4-a}
            }
            \subfigure[The best variable-length prefix-free code for $X^2$]{
                \includegraphics[width=.45\columnwidth]{A-8-P-4-b.pdf}
                \label{A-8-P-4-b}
            }
            \caption{The best variable-length prefix-free code schemes.}
        \end{figure}
    \end{itemize}
\end{sol}

\begin{prob}[2.33]
    Perform an LZ77 parsing of the string \uline{00011101}0010101100. Assume a window of length $W=8$; the initial window is underlined above. You should parse the rest of the string using the Lempel-Ziv algorithm.
\end{prob}
\begin{sol}
    The rest of the string can be parsed as
    \[
        \uline{00011101}\overbrace{001}^{u=7,n=3}\overbrace{0101}^{u=2,n=4}\overbrace{100}^{u=8,n=3},
    \]
    whose corresponding encoded sequence is
    \[
        011\;111\;00100\;010\;011\;000.
    \]
\end{sol}

\begin{prob}[4.35 Aliasing]
    The following exercise is designed to illustrate the sampling of an approximately baseband waveform. To avoid messy computation, we look at a waveform baseband-limited to $3/2$ which is sampled at rate $1$ (i.e. sampled at only $1/3$ the rate that it should be sampled at). In particular, let $u(t)=\sinc(3t)$.
    \begin{itemize}
        \item[(a)] Sketch $\hat{u}(f)$. Sketch the function $\hat{v}_m(f)=\rect(f-m)$ for each integer $m$ such that $v_m(f)\neq 0$. Note that $\hat{u}(f)=\sum_m\hat{v}_m(f)$.
        \item[(b)] Sketch the inverse transforms $v_m(t)$ (real and imaginary parts if complex).
        \item[(c)] Verify directly from the equations that $u(t)=\sum v_m(t)$. [Hint. This is easier if you express the sine part of the sinc function as a sum of complex exponentials.]
        \item[(d)] Verify the sinc-weighted sinusoid expansion, (4.73). (There are only three nonzero terms in the expansion.)
        \item[(e)] For the approximation $s(t)=u(0)\sinc(t)$, find the energy in the difference between $u(t)$ and $s(t)$ and interpret the terms.
    \end{itemize}
\end{prob}
\begin{sol}
    \begin{itemize}
        \item[(a)] The fourier transforms of $u(t)$
        \begin{align}
            \hat{u}(f)=\mathscr{F}[u(t)]=\frac{1}{3}\rect\left(\frac{f}{3}\right),
        \end{align}
        as shown in figure \ref{A-8-P-6-u(f)}.
        \begin{figure}[H]
            \centering
            \includegraphics[width=.4\columnwidth]{A-8-P-6-u(f).eps}
            \caption{$\hat{u}(f)$}
            \label{A-8-P-6-u(f)}
        \end{figure}
        The segment functions are
        \begin{align}
            \hat{v}_0(f)=&\frac{1}{3}\rect(f),\\
            \hat{v}_1(f)=&\frac{1}{3}\rect(f-1),\\
            \hat{v}_{-1}(f)=&\frac{1}{3}\rect(f+1),
        \end{align}
        as shown in figure \ref{A-8-P-6-vm(f)}.
        \begin{figure}[H]
            \centering
            \includegraphics[width=.99\columnwidth]{A-8-P-6-vm(f).eps}
            \caption{$\hat{v}_{m}(f)$ for $m=0,\pm 1$.}
            \label{A-8-P-6-vm(f)}
        \end{figure}
        \item[(b)] The inverse transform of $\hat{v}_0(f)$ is
        \begin{align}
            v_0(t)=\mathscr{F}^{-1}[\hat{v}_0(t)]=\frac{1}{3}\sinc(t).
        \end{align}
        as shown in figure \ref{A-8-P-6-v0(t)}.
        \begin{figure}[H]
            \centering
            \includegraphics[width=.4\columnwidth]{A-8-P-6-v0(t).eps}
            \caption{$v_0(t)$.}
            \label{A-8-P-6-v0(t)}
        \end{figure}
        The inverse transform of $\hat{v}_1(f)$ is
        \begin{align}
            v_1(t)=\mathscr{F}^{-1}[\hat{v}_1(t)]=\frac{1}{3}\sinc(t)e^{2\pi it},
        \end{align}
        whose real part is
        \begin{align}
            \re[v_1(t)]=\frac{1}{3}\sinc(t)\cos(2\pi t),
        \end{align}
        and imaginary part is
        \begin{align}
            \im[v_1(t)]=\frac{1}{3}\sinc(t)\sin(2\pi t),
        \end{align}
        as shown in figure \ref{A-8-P-6-v1(t)}.
        \begin{figure}[H]
            \centering
            \includegraphics[width=.9\columnwidth]{A-8-P-6-v1(t).eps}
            \caption{Real part and imaginary part of $v_1(t)$.}
            \label{A-8-P-6-v1(t)}
        \end{figure}
        The inverse transform of $\hat{v}_{-1}(f)$ is
        \begin{align}
            v_{-1}(t)=\mathscr{F}^{-1}[\hat{v}_{-1}(t)]=\frac{1}{3}\sinc(t)e^{-2\pi it},
        \end{align}
        whose real part is
        \begin{align}
            \re[v_{-1}(t)]=\frac{1}{3}\sinc(t)\cos(2\pi t),
        \end{align}
        and imaginary part is
        \begin{align}
            \im[v_{-1}(t)]=-\frac{1}{3}\sinc(t)\sin(2\pi t),
        \end{align}
        as shown in figure \ref{A-8-P-6-v-1(t)}.
        \begin{figure}[H]
            \centering
            \includegraphics[width=.9\columnwidth]{A-8-P-6-v-1(t).eps}
            \caption{Real part and imaginary part of $v_{-1}(t)$.}
            \label{A-8-P-6-v-1(t)}
        \end{figure}
        \item[(c)] 
        \begin{align}
            \notag\sum_mv_m(t)=&v_0(t)+v_1(t)+v_{-1}(t)=\frac{1}{3}\frac{e^{\pi it}-e^{-\pi it}}{2\pi it}+\frac{1}{3}\frac{e^{\pi it}-e^{-\pi it}}{2\pi it}e^{2\pi it}+\frac{1}{3}\frac{e^{\pi it}-e^{-\pi it}}{2\pi it}e^{-2\pi it}\\
            =&\frac{1}{3}\frac{e^{3\pi it}-e^{-3\pi it}}{2\pi it}=\frac{\sin(3\pi t)}{3\pi t}=\sinc(3t)=u(t)
        \end{align}
        \item[(d)] Sampling period $T=1$,
        \begin{align}
            \notag\sum_{m,k}v_m(kT)\sinc\left(\frac{t}{T}-k\right)e^{2\pi imt/T}=&\sum_{m=0,\pm 1}v_m(0)\sinc(t)e^{2\pi imt}=\frac{1}{3}\sin(t)+\frac{1}{3}\sinc(t)e^{2\pi it}+\frac{1}{3}\sinc(t)e^{-2\pi it}\\
            \notag=&\frac{1}{3}\frac{e^{\pi it}-e^{-\pi it}}{2\pi it}+\frac{1}{3}\frac{e^{\pi it}-e^{-\pi it}}{2\pi it}e^{2\pi it}+\frac{1}{3}\frac{e^{\pi it}-e^{-\pi it}}{2\pi it}e^{-2\pi it}\\
            \notag=&\frac{1}{3}\frac{e^{3\pi it}-e^{-3\pi it}}{2\pi it}=\frac{\sin(3\pi t)}{3\pi t}=\sinc(3t)=u(t).
        \end{align}
        \item[(e)] The approximation function
        \begin{align}
            s(t)=u(0)\sinc(t)=\sinc(t).
        \end{align}
        Using Parseval's Theorem, the energy between $u(t)$ and $s(t)$ is
        \begin{align}
            \notag\int_{-\infty}^{+\infty}\abs{u(t)-s(t)}^2\,\mathrm{d}t=&\int_{-\infty}^{+\infty}\abs{\hat{u}(f)-\hat{s}(f)}=\int_{-\infty}^{+\infty}\abs{\frac{1}{3}\rect\left(\frac{f}{3}\right)-\rect\left(f\right)}^2\,\mathrm{d}f=\frac{2}{3}.
        \end{align}
        which shows that $s(t)$ is not a very good approximation of $u(t)$, i.e., $s(t)$ and $u(t)$ are not $\mathcal{L}_2$ equivalent.
    \end{itemize}
\end{sol}
\end{document}