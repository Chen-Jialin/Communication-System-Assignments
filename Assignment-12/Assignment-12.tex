\documentclass{assignment}
\UsingEnglish
\ProjectInfos*{Intro to Communication System}{EE140}{Fall, 2020}{Assignment 12}{Due time : Nil}{陈稼霖}{45875852}
\begin{document}
\begin{prob}[8.9, otrhogonal signal sets; continuation of Exercise 8.8]
    Consider a set $\mathcal{A}=\{\bm{a}_m,0\leq m\leq M-1\}$ of $M$ orthogonal vectors in $\mathbb{R}^M$ with equal energy $E$.
    \begin{itemize}
        \item[(a)] Use the union bound to show that $\mathrm{Pr}(e)$, using ML detection, is bounded by
        \[
            \mathrm{Pr}(e)\leq(M-1)Q(\sqrt{E/N_0}).
        \]
        \item[(b)] Let $M\rightarrow\infty$ with $E_b=E/\log M$ held constant. Using the upperbound for $Q(x)$ in Exercise 8.7(b), show that if $E_b/N_0>2\ln 2$, then $\lim_{M\rightarrow\infty}\mathrm{Pr}(e)=0$. How close is this to the ultimate Shannon limit on $E_b/N_0$? What is the limit of the spectral efficiency $\rho$?
    \end{itemize}
\end{prob}
\begin{sol}
    \begin{itemize}
        \item[(a)] Suppose $\bm{U}=\bm{a}_m$ is transmitted and $\bm{V}$ is received. Using ML detection, if $\norm{\bm{V}-\bm{a}_j}\leq\norm{\bm{V}-\bm{a}_k}\forall k$, we determine that $\tilde{\bm{U}}=a_j$. If $\exists j$ s.t. $\norm{\bm{V}-\bm{a}_j}\leq\norm{\bm{V}-\bm{a}_m}$, then an error occurs. Use $A_{jm}$ to denote the event $\norm{\bm{V}-\bm{a}_j}<\norm{\bm{V}-\bm{a}_m}$. The error probability is
        \begin{align}
            \mathrm{Pr}(e\vert\bm{U}=\bm{a}_m)=\mathrm{Pr}(\cup_{j\neq m}A_{jm}\vert\bm{U}=\bm{a}_m)\leq\sum_{j\neq m}\mathrm{Pr}(A_{jm}\vert\bm{U}=\bm{a}_m).
        \end{align}
        Suppose that $\bm{V}=\bm{U}+\bm{Z}$, where $\bm{Z}$ is Gaussian noise with variance of $N_0/2$ per dimension. Since $\{\bm{a}_m,0\leq m\leq M-1\}$ are orthogonal with equal energy $E$,
        \begin{align}
            \mathrm{Pr}(A_{jm}\vert\bm{U}=\bm{a}_m)=Q\left(\frac{\norm{\bm{a}_j-\bm{a}_m}/2}{\sqrt{N_0/2}}\right)=Q\left(\sqrt{\frac{\norm{\bm{a}_j-\bm{a}_m}^2}{2N_0}}\right)=Q\left(\sqrt{\frac{E}{N_0}}\right).
        \end{align}
        Therefore,
        \begin{align}
            \mathrm{Pr}(e\vert\bm{U}=\bm{a}_m)\leq(M-1)Q(\sqrt{E/N_0}).
        \end{align}
        \item[(b)] According to Exercise 8.7, for $x\geq 0$,
        \begin{align}
            Q(x)\leq\frac{1}{2}\exp\left(-\frac{x^2}{2}\right),
        \end{align}
        so
        \begin{align}
            0<\mathrm{Pr}(e)\leq\frac{M-1}{2}\exp\left(-\frac{E}{2N_0}\right)=\frac{M-1}{2}\exp\left(-\frac{E_b\log_2M}{2N_0}\right)=\frac{M-1}{2}\left(\frac{1}{M}\right)^{\frac{E_b}{2N_0\ln 2}}.
        \end{align}
        If $E_b/N_0>2\ln 2$, then $\frac{E_b}{2N_0\ln 2}>1$. Let $M\rightarrow\infty$,
        \begin{align}
            \lim_{M\rightarrow\infty}\mathrm{Pr}(e)=0.
        \end{align}
        The limit of the spectral efficiency $\rho$ is
        \begin{align}
            \lim_{M\rightarrow\infty}\rho=\lim_{M\rightarrow\infty}\frac{2\log_2M}{M}=0.
        \end{align}
    \end{itemize}
\end{sol}

\begin{prob}[8.11]
    Section 8.3.4 discusses detection for binary complex vectors in WGN by viewing complex $n$-dimensional vectors as $2n$-dimensional real vectors. Here you will treat the vectors directly as $n$-dimension complex vectors. Let $\bm{Z}=(Z_1,\cdots,Z_n)^T$ be a vector of complex idd Gaussian rvs with iid real and imaginary parts, each $\mathcal{N}(0,N_0/2)$. The input $\bm{U}$ is binary antipodal, taking on values $\bm{a}$ and $-\bm{a}$. The observation $\bm{V}$ is $\bm{U}+\bm{Z}$.
    \begin{itemize}
        \item[(a)] The probability density of $\bm{Z}$ is given by
        \[
            f_{\bm{Z}}(z)=\frac{1}{(\pi N_0)^n}\exp\sum_{j=1}^n\frac{-\abs{z_j}^2}{N_0}=\frac{1}{(\pi N_0)^n}\exp\frac{-\norm{z}^2}{N_0}.
        \]
        Explain what this probability density represents (i.e. probability per unit what?)
        \item[(b)] Give expression for $f_{\bm{V}\vert\bm{U}}(\bm{v}\vert\bm{a})$ and $f_{\bm{V}\vert\bm{U}}(\bm{v}\vert-\bm{a})$.
        \item[(c)] Show that the log likelihood ratio for the observation $\bm{v}$ is given by
        \[
            \mathrm{LLR}(\bm{v})=\frac{-\norm{\bm{v}-\bm{a}}^2+\norm{\bm{v}+\bm{a}}^2}{N_0}.
        \]
        \item[(d)] Explain why this implies that ML detection is minimum distance detection (defining the distance between two complex vectors as the norm of their difference).
        \item[(e)] Show that $\mathrm{LLR}(\bm{v})$ can also be written as $4\re[\langle\bm{v},\bm{a}\rangle]/N_0$.
        \item[(f)] The appearance of the real part, $\re[\langle\bm{v},\bm{a}\rangle]$, in part (e) is surprising. Point out why log likelihood ratios must be real. Also explain why replacing $\re[\langle\bm{v},\bm{a}\rangle]$ by $\abs{\langle\bm{v},\bm{a}\rangle}$ in the above expression would give a nonsensical result in the ML test.
        \item[(g)] Does the set of points $\{\bm{v}:\text{LLR}(\bm{v})=0\}$ form a complex vector space?
    \end{itemize}
\end{prob}
\begin{sol}
    \begin{itemize}
        \item[(a)] The probability density represents the probability per unit volume in $2n$-dimensional real vector space.
        \item[(b)] 
        \begin{align}
            f_{\bm{V}\vert\bm{U}}(\bm{v}\vert\bm{a})=&\frac{1}{(\pi N_0)^n}\exp\frac{-\norm{\bm{v}-\bm{a}}^2}{N_0},\\
            f_{\bm{V}\vert\bm{U}}(\bm{v}\vert-\bm{a})=&\frac{1}{(\pi N_0)^n}\exp\frac{-\norm{\bm{v}+\bm{a}}^2}{N_0}.
        \end{align}
        \item[(c)] The log likelihood ratio for the observation $\bm{v}$ is given by
        \begin{align}
            \mathrm{LLR}(\bm{v})=\ln\frac{f_{\bm{V}\vert\bm{U}}(\bm{v}\vert\bm{a})}{f_{\bm{V}\vert\bm{U}}(\bm{v}\vert-\bm{a})}=\ln\left[\exp\frac{-\norm{\bm{v}-\bm{a}}^2+\norm{\bm{v}+\bm{a}}^2}{N_0}\right]=\frac{-\norm{\bm{v}-\bm{a}}^2+\norm{\bm{v}+\bm{a}}^2}{N_0}.
        \end{align}
        \item[(d)] The ML detection rule is
        \begin{align}
            \mathrm{LLR}(\bm{v})=\frac{-\norm{\bm{v}-\bm{a}}^2+\norm{\bm{v}+\bm{a}}^2}{N_0}\underset{\tilde{U}=-\bm{a}}{\overset{\tilde{U}=\bm{a}}{\gtreqless}}0.
        \end{align}
        Since $\norm{\bm{v}-\bm{a}}$ is the distance between $\bm{v}$ and $\bm{a}$ and $\norm{\bm{v}+\bm{a}}^2$ is the distance between $\bm{v}$ and $-\bm{a}$, if the distance between $\bm{v}$ and $\bm{a}$ is less than that between $\bm{v}$ and $-\bm{a}$, ML detection gives $\tilde{U}=\bm{a}$, otherwise, giving $\tilde{U}=-\bm{a}$. Therefore, this $\mathrm{LLR}(\bm{v})$ implies that ML detection is minimum distance detection.
        \item[(e)] $\mathrm{LLR}$ can be written as
        \begin{align}
            \mathrm{LLR}(\bm{v})=\frac{2\langle\bm{v},\bm{a}\rangle+\langle\bm{a},\bm{v}\rangle}{N_0}=\frac{4\re[\langle\bm{v},\bm{a}\rangle]}{N_0}.
        \end{align}
        \item[(f)] The likelihood ratio is the ratio of two positive numbers, so its logarithm $\mathrm{LLR}(\bm{v})$ is also a real number. Consider the one-dimensional case with $\bm{a}=1$. The noise in the imaginary direction is irrelevant and only the real component of noise is relevant.
        \item[(g)] \uline{No}. Consider the one-dimensional case with $\bm{a}=1$. The set $\{\bm{v}:\mathrm{LLR}(\bm{v})\}$ is pure imaginary numbers, which is not closed under scalar multiplication by complex number.
    \end{itemize}
\end{sol}
\end{document}